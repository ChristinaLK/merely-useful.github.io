# Publishing {#publish}

## Questions {#publish-questions}

```{r, child="questions/publish.md"}
```

## Objectives {#publish-objectives}

```{r, child="objectives/publish.md"}
```

## Introduction {#publish-intro}

This lesson looks at what should be included in reports and how best to do that.
We use the generic term "report" to include research papers,
summaries for clients,
and everything else that is shorter than a book and is going to be read by someone else.

Our motivation is summed up in this quotation:

> An article about computational science in a scientific publication is *not* the scholarship itself,
> it is merely *advertising* of the scholarship.
> The actual scholarship is the complete software development environment
> and the complete set of instructions which generated the figures.
>
> -- Jonathan Buckheit and David Donoho, paraphrasing Jon Claerbout, in @Buck1995

As the quote suggests,
modern publishing involves much more than simply producing a written report.
It involves providing readers with the data underpinning the report,
as well as any code written in analysing the data.
While the definition of data (and its associated metadata)
is relatively easy to understand,
code can come in many different forms.
Here we distinguish between “analysis code” written
solely for the purpose of the report (e.g. to produce a figure)
and “analysis software” that is formally packaged and released for use by a wider audience.
Of course, in reality the analysis code/software written during the process of preparing
a report can often lie on a continuum between those two definitions. 
(And in some cases a new dataset or analysis software will be the primary output,
as opposed to the written report.)

While some written reports, datasets, software packages and/or analysis code
can't be published without violating personal or commercial confidentiality,
every researcher's default should be to make all these components
of their work as widely available as possible.
That means publishing it in an open access venue (Chapter \@ref(inclusive))
so that people who aren't in academia can find and access it.


## Identification {#publish-identifiers}

Before publishing anything,
we need to understand the systems used to identify works
and their individual authors. 

A [Digital Object Identifier](glossary.html#doi) (DOI)
is a unique identifier for a particular version of a particular digital artifact
such as a report, a dataset, or a piece of software.
DOIs are written as `doi:prefix/suffix`,
but you will often also see them represented as URLs like `http://dx.doi.org/prefix/suffix`.
In order to be allowed to issue a DOI,
online platforms (e.g. academic journals, data archives)
must guarantee a certain level of security, longevity and access.

An [ORCID](glossary.html#orcid) is an Open Researcher and Contributor ID.
You can [get an ORCID][orcid] for free,
and you should include it in publications
because people's names and affiliations change over time.


## Publishing a written document {#publish-written}

The best option for publishing a written report (with a platform that issues a DOI)
depends on the context.
For academic, peer-reviewed research papers,
numerous open access journals have popped up in recent years.
Many formally closed-access journals now also offer an open access option (for a fee).
 
Another option is to publish with an online pre-print server
(e.g. [bioRxiv](https://www.biorxiv.org/), [arXiv](https://arxiv.org/)).  
A preprint is a version of an academic research paper that precedes formal peer review
and publication in a peer-reviewed journal.
The preprint may be available, often as a non-typeset version available free,
before and/or after a paper is published in a journal. 

Online writing platforms such as [Authorea](https://authorea.com/) are also an option.
It allows a report to be openly viewable on the web throughout the entire writing process.
At the end of the process Authorea can issue its own DOI for the report,
or the text can be exported in the format required for submission to an academic journal.

Finally, online platforms such as [Figshare](https://figshare.com) and [Zenodo](https://zenodo.org/)
are a place where any research outputs (reports, datasets, code, supplementary figures)
can be published with a DOI.
It is common for people to upload reports to these platforms so that they can be
easily accessed by others.
  

## Publishing data {#publish-data}

The first step in publishing the data associated with a report
is to determine what (if anything) needs to be published.

If the report involved the analysis of a publicly available dataset
that is maintained and documented by a third party (e.g. open government data),
then it's likely that no data publishing is required.
The report simply needs to document where to access the data
and what version was analyzed,
along with any software and code used to download and process the data (see next section).
In other words, it's not necessary to re-publish a duplicate of the original dataset
if it's already accessible elsewhere.

Strictly speaking, it's not necessary to publish any data files
produced during the analysis of a publicly available dataset either,
since readers have access to the original data and the software/code used to process it.
Having said that, it can be advantageous to publish processed data that difficult to reproduce
(e.g. it might require access to a supercomputing facility to run the code)
and/or represents a derived quantity with high re-use potential.
For instance, it would be worthwhile to publish 
an estimate of the global average surface temperature derived from
a large database of weather observations,
because a simple metric of global warming could be useful in many subsequent studies.

If a report involves the generation of a new dataset
(e.g. observations collected during a field experiment),
then clearly that dataset needs to be published.
This section describes how to go about doing that.


### What is the most useful way to share my data? 

Making data useful to other people (including your future self)
is one of the best investments you can make.
The simple version of how to do this is:

-   Always use [tidy data](glossary.html#tidy-data).
-   Include keywords describing the data in the project's `README.md`
    so that they appear on its home page and can easily be found by search engines.
-   Give every dataset and every report a unique identifier (Section \@ref(publish-identifiers)).
-   Put data in open repositories (Section \@ref(publish-data)).
-   Use well-known formats like CSV and HDF5.
-   Include an explicit license in every project and every dataset.
-   Include units and other metadata.

The last point is often the hardest for people to implement,
since many researchers have never seen a well-documented dataset.
We draw inspiration from the data catalog included in [the repository][womens-pockets-data]
for the article "[Women's Pockets Are Inferior][womens-pockets]"
and include a file `./data/README.md` in every project
that looks like this:

```
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   `Infant_HIV_Testing_2017.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2009-2017
    -   `infant_hiv.csv`
        -   What is this?: CSV export from `Infant_HIV_Testing_2017.xlsx`
    -   Notes
        -   Data is not tidy: some rows are descriptive comments, others are blank separators between sections, and column headers are inconsistent.
        -   Use `tidy_infant_hiv()` to tidy this data.
-   Maternal health indicators disaggregated by age
    -   `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/maternal-health-data/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2000-2014
    -   `at_health_facilities.csv`
        -   What is this?: percentage of births at health facilities by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `c_sections.csv`
        -   What is this?: percentage of Caesarean sections by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `skilled_attendant_at_birth.csv`
        -   What is this?: percentage of births with skilled attendant present by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   Notes
        -   Data is not tidy: some rows are descriptive comments, others are blank separators between sections, and column headers are inconsistent.
        -   Use `tidy_maternal_health_adolescents()` to tidy this data.
```

The catalog above doesn't include column headers or units because the data isn't tidy.
It *does* include the names of the functions used to reformat that data,
and `./results/README.md` then includes the information that users will want.
One section of that file is shown below:

```
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   infant_hiv.csv
      -   What is this?: tidied version of CSV export from spreadsheet.
      -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
      -   Last Modified: September 2018
      -   Contact: Greg Wilson <greg.wilson@rstudio.com>
      -   Spatial Applicability: global
      -   Temporal Applicability: 2009-2017
      -   Generated By: scripts/tidy-24.R

| Header   | Datatype | NA    | Description                                 |
|----------|----------|-------|---------------------------------------------|
| country  | char     | false | ISO3 country code of country reporting data |
| year     | integer  | false | year CE for which data reported             |
| estimate | double   | true  | estimated percentage of measurement         |
| hi       | double   | true  | high end of range                           |
| lo       | double   | true  | low end of range                            |
```

Note that this catalog includes both units and whether or not a field can be NA.
Note also that calling a field "NA" is asking for trouble...

### What standards of data sharing should I aspire to? 

The [FAIR Principles][go-fair] describe what research data should look like.
They are [still aspirational](https://www.natureindex.com/news-blog/what-scientists-need-to-know-about-fair-data)
for most researchers,
but they tell us what to aim for.
The most immediately important elements of the FAIR Principles are outlined below.

#### Data should be *findable*.

The first step in using or re-using data is to find it.
You can tell you've done this if:

1.  (Meta)data is assigned a globally unique and persistent identifier (Section \@ref(publish-identifiers)).
2.  Data is described with rich metadata (like the catalog shown above).
3.  Metadata clearly and explicitly includes the identifier of the data it describes.
4.  (Meta)data is registered or indexed in a searchable resource,
    such as the data sharing platforms described in Section \@ref(publish-data).

#### Data should be *accessible*.

You can't use data if you don't have access to it.
In practice,
this rule means the data should be openly accessible (the preferred solution)
or that authenticating in order to view or download it should be free.
You can tell you've done this if:

1.  (Meta)data is retrievable by its identifier using a standard communications protocol like HTTP.
2.  Metadata is accessible even when the data is no longer available.

#### Data should be *interoperable*.

Data usually needs to be integrated with other data,
which means that tools need to be able to process it.
You can tell you've done this if:

1.  (Meta)data uses a formal, accessible, shared, and broadly applicable language for knowledge representation
2.  (Meta)data uses vocabularies that follow FAIR principles
3.  (Meta)data includes qualified references to other (meta)data

#### Data should be *reusable*.

This is the ultimate purpose of the FAIR Principles and much other work.
You can tell you've done this if:

1.  Meta(data) is described with accurate and relevant attributes.
2.  (Meta)data is released with a clear and accessible data usage license.
3.  (Meta)data has detailed [provenance](glossary.html#provenance).
4.  (Meta)data meets domain-relevant community standards.

### How and where do I publish the data?

Small datasets (i.e., anything under 500 MB) can be stored in version control
using the conventions described in Chapter \@ref(project).
If the data is being used in several projects,
it may make sense to create one repository to hold only the data;
the R community refers to these as [data packages](glossary.html#data-package),
and they are often accompanied by small scripts to clean up and query the data.
Be sure to give the dataset an identifier as discussed in Section \@ref(publish-identifiers).

For medium-sized datasets (between 500 MB and 5 GB),
it's better to put the data on platforms like the [Open Science Framework][osf],
[Dryad][dryad],
and [Figshare][figshare].
Each of these will give the datasets identifiers;
those identifiers should be included in reports
along with scripts to download the data.
Big datasets (i.e., anything more than 5 GB)
may not be yours in the first place,
and probably need the attention of a professional archivist.
Any processed or intermediate data that takes a long time to regenerate
should probably be published as well using these same sizing rules;
all of this data should be given identifiers,
and those identifiers should be included in reports.

> **Data journals**
>
> While archiving data at a site like Dryad or Figshare (following the FAIR Principles)
> is usually the end of the data publishing process, 
> there is the option of publishing a journal paper to describe the dataset in detail.
> Some research disciplines have journals devoted
> to describing particular types of data
> (e.g. [Geoscience Data Journal](https://rmets.onlinelibrary.wiley.com/journal/20496060))
> and there are also generic data journals
> (e.g. [Scientific Data](https://www.nature.com/sdata/)).


## Publishing software {#publish-software}

In the preceding chapters we have learned how to document and package software
so that it can be installed and used by others.
The final step in this process is publication.

It is common practice to have the code associated with a software package
openly available on a hosting service such as GitHub (or GitLab or Bitbucket).
These hosting services are not only a convenient place for people to ask questions
and make contributions/improvements to the software,
they also have built-in functionality for managing the
release of new versions of the software.
One limitation of these sites, however,
is that they don’t guarantee persistent long term storage
(e.g. if you changed the name of your GitHub repository any URLs for
the existing repository would be broken).
Acknowledging this limitation,
GitHub provides [Zenodo integration](https://guides.github.com/activities/citable-code/)
for creating a DOI with each new software release.

> **Software journals**
>
> While creating a DOI using a site like Zenodo
> is often the end of the software publishing process,
> there is the option of publishing
> a journal paper to describe the software in detail.
> Some research disciplines have journals devoted
> to describing particular types of software
> (e.g. [Geoscientific Model Development](https://www.geoscientific-model-development.net/)),
> and there are also a number of generic software journals such as the
> [Journal of Open Research Software](https://openresearchsoftware.metajnl.com/) and
> [Journal of Open Source Software](https://joss.theoj.org/).


## Publishing code {#publish-code}

The final component that needs to be published is the analysis code.
Unlike analysis software that has been packaged and released for use by a wider audience,
analysis code is simply written to create the figures and tables presented in a given report.
In fact, such code would typically make use of analysis software written
by the wider data science community (e.g. matplotlib, ggplot)
as well discipline-specific packages written by colleagues or co-authors 
(e.g. [AstroPy](https://www.astropy.org/)).

Given that analysis code will typically leverage
a wide variety of existing software packages,
there's actually three separate items that need to be published:

1. A detailed description of the analysis software used
2. A copy of any analysis code written by the authors to produce the key results
   presented in the report
3. A description of the data processing steps taken in producing each key result
   (i.e. a step-by-step account of how the software and code were actually implemented)

### Software description

In order to document the software packages that were used,
the bare minimum you’d need to do is follow the advice of the Software Sustainability Institute.
They suggest documenting the name, version number, release date,
institution and DOI or URL of each software package,
which could be included in a supplementary text file.

While such a list means your software environment is now technically reproducible,
you’ve left it up to the reader to figure out how to get all those software packages
and libraries installed and playing together nicely.
In some cases this is fine
(e.g. it might be easy enough for a reader to install the handful of R packages you used),
but in other cases you might want to save the reader (and your future self)
the pain of software installation by making use of a tool
that can automatically install a specified software environment.
The simplest of these is conda, which I discussed in detail in a previous post.
It is primarily used for the management of Python packages,
but can be used for other software as well.
I install my complete environment with conda,
which includes non-Python command line utilities like the Climate Data Operators,
and then make that environment openly available on my channel at anaconda.org.
Beyond conda there are more complex tools like Docker and Nix,
which can literally install your entire environment (down to the precise operating system)
on a different machine.
There’s lots of debate (e.g. here)
about the potential and suitability of these tools as a solution to reproducible research,
but it’s fair to say that their complexity puts them out of reach
for most weather and climate scientists.


### Analysis code

The next supplementary item you’ll need to provide is a copy of the code
you wrote to execute those software packages.
For a very simple analysis that might consist of a single script for each key result (e.g. each figure),
but it’s more likely to consist of a whole library/collection of code containing many interconnected scripts.
The bare minimum you’d need to do to make your paper reproducible is to make an
instantaneous snapshot of that library (i.e. at the time of paper submission or acceptance)
available as supplementary material.

As with the software description,
this bare minimum ensures your paper is reproducible,
but it leaves a few problems for both you and the reader.
The first is that in order to provide an instantaneous snapshot,
you’d need to make sure that all your results were produced with
the latest version of your code library.
In many cases this isn’t practical 
(e.g. Figure 3 might have been generated five months ago and
you don’t want to re-run the whole time consuming process),
so you’ll probably want to manage your code library with a version control system like
Git, Subversion or Mercurial, so you can easily access previous versions.
If you’re using a version control system you might as well hook it up
to an external hosting service like GitHub or Bitbucket,
so you’ve got your code backed up elsewhere.
If you make your GitHub or Bitbucket repository publicly accessible then
readers can view the very latest version of your code
(in case you’ve made any improvements since publishing the paper),
as well as submit proposed updates or bug fixes via the useful interface
(which includes commenting, chat and code viewing features) that those websites provide.


### Data processing steps

A code library and software description on their own are not much use to a reader;
they also need to know how that code was used in generating the results presented.
The simplest way to do this is to make your scripts executable at the command line,
so you can then keep a record of the series of command line entries required to produce a given result.
Two of the most well known data analysis tools in the weather and climate sciences – the netCDF Operators (NCO) and Climate Data Operators (CDO) – do exactly this, storing that record in the global attributes of the output netCDF file. I’ve written a little Python library (cmdline-provenance) that generates these records, including keeping track of the corresponding version control revision number, so you know exactly which version of the code was executed.

As before, while these bare minimum log files ensure that your workflow is reproducible, they are not particularly comprehensible. Manually recreating workflows from these log files would be a tedious and time consuming process, even for just moderately complex analyses. To make things a little easier for the reader (and your future self), it’s a good idea to include a README file in your code library explaining the sequence of commands required to produce common/key results. You might also provide a Makefile that automatically builds and executes common workflows (Software Carpentry have a nice lesson on that topic). Beyond that the options get more complex, with workflow management packages like VisTrails providing a graphical interface that allows users to drag and drop the various components of their workflow.
Where to put these supplementary materials?

Following the steps above, you’ll be left with a text file (or environment.yml file exported from a conda environment) describing your software environment, a copy of your version controlled code library and various log files, README files and/or make files that describe your data processing steps. While you will have hopefully made some of these items openly available via anaconda.org and GitHub, those sites don’t guarantee persistent long term storage (e.g. if you changed the name of your GitHub repo any URLs in your paper would be broken). You therefore need to host your supplementary files with the journal, your home institution (e.g. Max Plank Institute for Meteorology provide a persistent digital storage facility for staff and students) or a site like Figshare or Zenodo. The latter sites have been specifically setup for archiving the “long tail” of research papers (e.g. supplementary figures, tables, code and data) and issue a DOI to those materials, meaning they guarantee a certain level of security, longevity and access (e.g. check out https://doi.org/10.6084/m9.figshare.1385387).



... a short computation section placed within the methods section of the report.
That computation section begins with a brief,
high-level summary of the major software packages that were used,
with citations provided to any papers dedicated to documenting that software.
Authors of scientific software are increasingly publishing overviews
of their software in journals like the Journal of Open Research Software and 
Journal of Open Source Software,
so it’s important to give them the academic credit they deserve.






### What software should I publish and how? 

Including a link to a GitHub repository in your publications is a good first step,
but it is only the first step.
Software changes over time,
and your scripts almost certainly depend on specific versions of other packages (Chapter \@ref(py-package)).
It therefore seems logical to include the exact version numbers of everything used in your analysis
in each publication,
but that's impractical.
Should you include the version of the compiler used to build the R or Python interpreter you used?
What about the operating system?
Is a hardware specification needed in case it turns out that
[the processor you used had a bug][pentium-div-bug]?
Some people now believe that researchers should put everything in a virtual machine and share that,
but a [Docker][docker] image is like a screenshot:
all the bits are there,
but it's a lot of work to get the information out.

Librarians, publishers, and regulatory bodies are still trying to find
useful answers to these questions.
For the moment,
the best advice we can give it to publish the version IDs of the standard software used in the analysis.
As Section \@ref(py-package-install) described,
you can get these automatically by running:

```shell
$ pip freeze > requirements.txt
```

For everything else,
you should write a script or create a rule in your project's Makefile (Chapter \@ref(automate)),
since the commands used to get version numbers will vary from tool to tool:

```make
## versions : dump versions of software.
versions :
        @echo '# Python packages'
        @pip freeze
        @echo '# dezply'
        @dezply --version
        @echo '# parajune'
        @parajune --status | head 1
```

The scripts, notebooks, and/or Makefiles used to produce results
tend to evolve at a different rate than data,
so they should get separate DOIs.
Depending on the size or complexity of the software you have written,
and whether you re-use it in multiple projects,
you may publish script by script
or create a zip file or tar file that includes everything.
For example,
the Makefile fragment below creates `~/archive/meow-2019-02-21.tgz`:

```make
ARCHIVE=${HOME}/archive
PROJECT=meow
TODAY=$(shell date "+%Y-%m-%d")
SCRIPTS=./Makefile ./bin/*.py ./bin/*.sh

## archive : create an archive of all the scripts used in this run
archive :
        @mkdir -p ${ARCHIVE}
        @tar zcf ${ARCHIVE}/${PROJECT}-${TODAY}.tgz
```

Finally,
it's imperative that you include the configuration files
and command-line parameters
that you used to generate particular runs.
If all of the program's parameters are in a configuration file (Chapter \@ref(configure)),
you can archive that.
Otherwise,
have your program print out its configuration parameters
and use `grep` or a script to extract from the logfile (Chapter \@ref(logging)).
If you can't do that because you're using someone else's software,
write a small shell script that logs the configuration
and then runs the program.





## Summary {#publish-summary}

FIXME: create concept map for publishing

## Exercises {#publish-exercises}

**ORCID**

If you don't already have an [ORCID][orcid],
go to the website and register now. 

If you do have an ORCID,
login at the website and make sure that your details
and publication record are up-to-date.


**A FAIR test**

An [online questionnaire](https://www.ands-nectar-rds.org.au/fair-tool)
for measuring the extent to which datasets are FAIR
has been created by the Australian Research Data Commons.

Take the questionnaire for a dataset you have published
or that you use often. 



## Key Points {#publish-keypoints}

```{r, child="keypoints/publish.md"}
```

```{r, child="./links.md"}
```
