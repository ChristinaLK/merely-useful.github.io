# Publishing {#publish}

## Questions {#publish-questions}

```{r, child="questions/publish.md"}
```

## Objectives {#publish-objectives}

```{r, child="objectives/publish.md"}
```

## Introduction {#publish-intro}

This lesson looks at what should be included in reports and how best to do that.
We use the generic term "report" to include research papers,
summaries for clients,
and everything else that is shorter than a book and is going to be read by someone else.

Our motivation is summed up in this quotation:

> An article about computational science in a scientific publication is *not* the scholarship itself,
> it is merely *advertising* of the scholarship.
> The actual scholarship is the complete software development environment
> and the complete set of instructions which generated the figures.
>
> -- Jonathan Buckheit and David Donoho, paraphrasing Jon Claerbout, in @Buck1995

As the quote suggests,
modern publishing involves much more than simply producing a written report.
It involves providing readers with the data underpinning the report,
as well as any code written in analysing the data.
While the definition of data (and its associated metadata)
is relatively easy to understand,
code can come in many different forms.
Here we distinguish between “analysis code” written
solely for the purpose of the report (e.g. to produce a figure)
and “analysis software” that is formally packaged and released for use by a wider audience.
Of course, in reality the analysis code/software written during the process of preparing
a report can often lie on a continuum between those two definitions. 
(And in some cases a new dataset or analysis software will be the primary output,
as opposed to the written report.)

While some written reports, datasets, software packages and/or analysis code
can't be published without violating personal or commercial confidentiality,
every researcher's default should be to make all these components
of their work as widely available as possible.
That means publishing it in an open access venue (Chapter \@ref(inclusive))
so that people who aren't in academia can find and access it.


## Identification {#publish-identification}

Before publishing anything,
we need to understand the systems used to identify works
and their individual authors. 

A [Digital Object Identifier](glossary.html#doi) (DOI)
is a unique identifier for a particular version of a particular digital artifact
such as a report, a dataset, or a piece of software.
DOIs are written as `doi:prefix/suffix`,
but you will often also see them represented as URLs like `http://dx.doi.org/prefix/suffix`.
In order to be allowed to issue a DOI,
online platforms (e.g. academic journals, data archives)
must guarantee a certain level of security, longevity and access.

An [ORCID](glossary.html#orcid) is an Open Researcher and Contributor ID.
You can [get an ORCID][orcid] for free,
and you should include it in publications
because people's names and affiliations change over time.


## Publishing a written document {#publish-written}

The best option for publishing a written report (with a platform that issues a DOI)
depends on the context.
For academic, peer-reviewed research papers,
numerous open access journals have popped up in recent years.
Many formally closed-access journals now also offer an open access option (for a fee).
 
Another option is to publish with an online pre-print server
(e.g. [bioRxiv](https://www.biorxiv.org/), [arXiv](https://arxiv.org/)).  
A preprint is a version of an academic research paper that precedes formal peer review
and publication in a peer-reviewed journal.
The preprint may be available, often as a non-typeset version available free,
before and/or after a paper is published in a journal. 

Online writing platforms such as [Authorea](https://authorea.com/) are also an option.
It allows a report to be openly viewable on the web throughout the entire writing process.
At the end of the process Authorea can issue its own DOI for the report,
or the text can be exported in the format required for submission to an academic journal.

Finally, online platforms such as [Figshare](https://figshare.com) and [Zenodo](https://zenodo.org/)
are a place where any research outputs (reports, datasets, code, supplementary figures)
can be published with a DOI.
It is common for people to upload reports to these platforms so that they can be
easily accessed by others.
  

## Publishing data {#publish-data}

The first step in publishing the data associated with a report
is to determine what (if anything) needs to be published.

If the report involved the analysis of a publicly available dataset
that is maintained and documented by a third party (e.g. open government data),
then it's likely that no data publishing is required.
The report simply needs to document where to access the data
and what version was analyzed,
along with any software and code used to download and process the data (see next section).
In other words, it's not necessary to re-publish a duplicate of the original dataset
if it's already accessible elsewhere.

Strictly speaking, it's not necessary to publish any data files
produced during the analysis of a publicly available dataset either,
since readers have access to the original data and the software/code used to process it.
Having said that, it can be advantageous to publish processed data that difficult to reproduce
(e.g. it might require access to a supercomputing facility to run the code)
and/or represents a derived quantity with high re-use potential.
For instance, it would be worthwhile to publish 
an estimate of the global average surface temperature derived from
a large database of weather observations,
because a simple metric of global warming could be useful in many subsequent studies.

If a report involves the generation of a new dataset
(e.g. observations collected during a field experiment),
then clearly that dataset needs to be published.
This section describes how to go about doing that.


### What is the most useful way to share my data? 

Making data useful to other people (including your future self)
is one of the best investments you can make.
The simple version of how to do this is:

-   Always use [tidy data](glossary.html#tidy-data).
-   Include keywords describing the data in the project's `README.md`
    so that they appear on its home page and can easily be found by search engines.
-   Give every dataset and every report a unique identifier (Section \@ref(publish-identifiers)).
-   Put data in open repositories (Section \@ref(publish-data)).
-   Use well-known formats like CSV and HDF5.
-   Include an explicit license in every project and every dataset.
-   Include units and other metadata.

The last point is often the hardest for people to implement,
since many researchers have never seen a well-documented dataset.
We draw inspiration from the data catalog included in [the repository][womens-pockets-data]
for the article "[Women's Pockets Are Inferior][womens-pockets]"
and include a file `./data/README.md` in every project
that looks like this:

```
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   `Infant_HIV_Testing_2017.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2009-2017
    -   `infant_hiv.csv`
        -   What is this?: CSV export from `Infant_HIV_Testing_2017.xlsx`
    -   Notes
        -   Data is not tidy: some rows are descriptive comments, others are blank separators between sections, and column headers are inconsistent.
        -   Use `tidy_infant_hiv()` to tidy this data.
-   Maternal health indicators disaggregated by age
    -   `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
        -   What is this?: Excel spreadsheet with summarized data.
        -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/maternal-health-data/>
        -   Last Modified: July 2018 (according to website)
        -   Contact: Greg Wilson <greg.wilson@rstudio.com>
        -   Spatial Applicability: global
        -   Temporal Applicability: 2000-2014
    -   `at_health_facilities.csv`
        -   What is this?: percentage of births at health facilities by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `c_sections.csv`
        -   What is this?: percentage of Caesarean sections by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   `skilled_attendant_at_birth.csv`
        -   What is this?: percentage of births with skilled attendant present by country, year, and mother's age
        -   Source(s): single sheet from `maternal_health_adolescents_indicators_April-2016_250d599.xlsx`
    -   Notes
        -   Data is not tidy: some rows are descriptive comments, others are blank separators between sections, and column headers are inconsistent.
        -   Use `tidy_maternal_health_adolescents()` to tidy this data.
```

The catalog above doesn't include column headers or units because the data isn't tidy.
It *does* include the names of the functions used to reformat that data,
and `./results/README.md` then includes the information that users will want.
One section of that file is shown below:

```
-   Infants born to women with HIV receiving an HIV test within two months of birth, 2009-2017
    -   infant_hiv.csv
      -   What is this?: tidied version of CSV export from spreadsheet.
      -   Source(s): UNICEF, <https://data.unicef.org/resources/dataset/hiv-aids-statistical-tables/>
      -   Last Modified: September 2018
      -   Contact: Greg Wilson <greg.wilson@rstudio.com>
      -   Spatial Applicability: global
      -   Temporal Applicability: 2009-2017
      -   Generated By: scripts/tidy-24.R

| Header   | Datatype | NA    | Description                                 |
|----------|----------|-------|---------------------------------------------|
| country  | char     | false | ISO3 country code of country reporting data |
| year     | integer  | false | year CE for which data reported             |
| estimate | double   | true  | estimated percentage of measurement         |
| hi       | double   | true  | high end of range                           |
| lo       | double   | true  | low end of range                            |
```

Note that this catalog includes both units and whether or not a field can be NA.
Note also that calling a field "NA" is asking for trouble...

### What standards of data sharing should I aspire to? 

The [FAIR Principles][go-fair] describe what research data should look like.
They are [still aspirational](https://www.natureindex.com/news-blog/what-scientists-need-to-know-about-fair-data)
for most researchers,
but they tell us what to aim for.
The most immediately important elements of the FAIR Principles are outlined below.

#### Data should be *findable*.

The first step in using or re-using data is to find it.
You can tell you've done this if:

1.  (Meta)data is assigned a globally unique and persistent identifier (Section \@ref(publish-identifiers)).
2.  Data is described with rich metadata (like the catalog shown above).
3.  Metadata clearly and explicitly includes the identifier of the data it describes.
4.  (Meta)data is registered or indexed in a searchable resource,
    such as the data sharing platforms described in Section \@ref(publish-data).

#### Data should be *accessible*.

You can't use data if you don't have access to it.
In practice,
this rule means the data should be openly accessible (the preferred solution)
or that authenticating in order to view or download it should be free.
You can tell you've done this if:

1.  (Meta)data is retrievable by its identifier using a standard communications protocol like HTTP.
2.  Metadata is accessible even when the data is no longer available.

#### Data should be *interoperable*.

Data usually needs to be integrated with other data,
which means that tools need to be able to process it.
You can tell you've done this if:

1.  (Meta)data uses a formal, accessible, shared, and broadly applicable language for knowledge representation
2.  (Meta)data uses vocabularies that follow FAIR principles
3.  (Meta)data includes qualified references to other (meta)data

#### Data should be *reusable*.

This is the ultimate purpose of the FAIR Principles and much other work.
You can tell you've done this if:

1.  Meta(data) is described with accurate and relevant attributes.
2.  (Meta)data is released with a clear and accessible data usage license.
3.  (Meta)data has detailed [provenance](glossary.html#provenance).
4.  (Meta)data meets domain-relevant community standards.

### How and where do I publish the data?

Small datasets (i.e., anything under 500 MB) can be stored in version control
using the conventions described in Chapter \@ref(project).
If the data is being used in several projects,
it may make sense to create one repository to hold only the data;
the R community refers to these as [data packages](glossary.html#data-package),
and they are often accompanied by small scripts to clean up and query the data.
Be sure to give the dataset an identifier as discussed in Section \@ref(publish-identifiers).

For medium-sized datasets (between 500 MB and 5 GB),
it's better to put the data on platforms like the [Open Science Framework][osf],
[Dryad][dryad],
and [Figshare][figshare].
Each of these will give the datasets identifiers;
those identifiers should be included in reports
along with scripts to download the data.
Big datasets (i.e., anything more than 5 GB)
may not be yours in the first place,
and probably need the attention of a professional archivist.
Any processed or intermediate data that takes a long time to regenerate
should probably be published as well using these same sizing rules;
all of this data should be given identifiers,
and those identifiers should be included in reports.


## Publishing software {#publish-software}

TODO

We can use [Zenodo][zenodo] to [get DOIs for free][github-zenodo-tutorial]:

1.  Log in to [Zenodo][zenodo] using your GitHub ID.
    (The first time you do this,
    you will have to authorize the application so that Zenodo can read data from your GitHub repositories.)
    Once you have logged in,
    pick a repository and click the "On" button.
2.  Go back to that GitHub repository and go to the "Releases" tab.
    Create a new release,
    give it a version number (discussed in Section \@ref(branches-tag)),
    and then click "Publish release".
3.  Finally, go back to Zenodo
    and look under the "Upload" tab for a new upload corresponding to this release.
    Once it appears,
    fill in the required information and publish it.
    A Zenodo DOI badge will automatically appear on the GitHub project's site,
    and anyone with the DOI will be able to find it.

FIXME: screenshots and a diagram showing how this process works


## Publishing code {#publish-code}

TODO

... a short computation section placed within the methods section of the report.
That computation section begins with a brief,
high-level summary of the major software packages that were used,
with citations provided to any papers dedicated to documenting that software.
Authors of scientific software are increasingly publishing overviews
of their software in journals like the Journal of Open Research Software and 
Journal of Open Source Software,
so it’s important to give them the academic credit they deserve.

Following this high level summary,
the computation section points the reader to three key supplementary items:

- A more detailed description of the software used
- A copy of any code written by the authors to produce the key results
- A description of the data processing steps taken in producing each key result
  (i.e. a step-by-step account of how the software and code were actually used)

### What software should I publish and how? 

Including a link to a GitHub repository in your publications is a good first step,
but it is only the first step.
Software changes over time,
and your scripts almost certainly depend on specific versions of other packages (Chapter \@ref(py-package)).
It therefore seems logical to include the exact version numbers of everything used in your analysis
in each publication,
but that's impractical.
Should you include the version of the compiler used to build the R or Python interpreter you used?
What about the operating system?
Is a hardware specification needed in case it turns out that
[the processor you used had a bug][pentium-div-bug]?
Some people now believe that researchers should put everything in a virtual machine and share that,
but a [Docker][docker] image is like a screenshot:
all the bits are there,
but it's a lot of work to get the information out.

Librarians, publishers, and regulatory bodies are still trying to find
useful answers to these questions.
For the moment,
the best advice we can give it to publish the version IDs of the standard software used in the analysis.
As Section \@ref(py-package-install) described,
you can get these automatically by running:

```shell
$ pip freeze > requirements.txt
```

For everything else,
you should write a script or create a rule in your project's Makefile (Chapter \@ref(automate)),
since the commands used to get version numbers will vary from tool to tool:

```make
## versions : dump versions of software.
versions :
        @echo '# Python packages'
        @pip freeze
        @echo '# dezply'
        @dezply --version
        @echo '# parajune'
        @parajune --status | head 1
```

The scripts, notebooks, and/or Makefiles used to produce results
tend to evolve at a different rate than data,
so they should get separate DOIs.
Depending on the size or complexity of the software you have written,
and whether you re-use it in multiple projects,
you may publish script by script
or create a zip file or tar file that includes everything.
For example,
the Makefile fragment below creates `~/archive/meow-2019-02-21.tgz`:

```make
ARCHIVE=${HOME}/archive
PROJECT=meow
TODAY=$(shell date "+%Y-%m-%d")
SCRIPTS=./Makefile ./bin/*.py ./bin/*.sh

## archive : create an archive of all the scripts used in this run
archive :
        @mkdir -p ${ARCHIVE}
        @tar zcf ${ARCHIVE}/${PROJECT}-${TODAY}.tgz
```

Finally,
it's imperative that you include the configuration files
and command-line parameters
that you used to generate particular runs.
If all of the program's parameters are in a configuration file (Chapter \@ref(configure)),
you can archive that.
Otherwise,
have your program print out its configuration parameters
and use `grep` or a script to extract from the logfile (Chapter \@ref(logging)).
If you can't do that because you're using someone else's software,
write a small shell script that logs the configuration
and then runs the program.





## Summary {#publish-summary}

FIXME: create concept map for publishing

## Exercises {#publish-exercises}

**ORCID**

If you don't already have an [ORCID][orcid],
go to the website and register now. 

If you do have an ORCID,
login at the website and make sure that your details
and publication record are up-to-date.


**A FAIR test**

An [online questionnaire](https://www.ands-nectar-rds.org.au/fair-tool)
for measuring the extent to which datasets are FAIR
has been created by the Australian Research Data Commons.

Take the questionnaire for a dataset you have published
or that you use often. 



## Key Points {#publish-keypoints}

```{r, child="keypoints/publish.md"}
```

```{r, child="./links.md"}
```
